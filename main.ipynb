{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from functools import partial\n",
    "import torch\n",
    "\n",
    "from datasets import DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments\n",
    "\n",
    "from conll_parser import read_conll_file, convert_to_hf_dataset\n",
    "from ner_pipeline_utils import compute_classification_metrics, tokenize_and_align_labels, data_collator\n",
    "from viz import print_sample_data, print_hf_dataset, print_tokenized_dataset, extract_named_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Current device index: 0\n",
      "Device name: NVIDIA GeForce RTX 2070\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Current device index:\", torch.cuda.current_device())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Paths and Training Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths and configurations\n",
    "data_path = \"D:/files/conll2003\"  # Path to CoNLL 2003 dataset\n",
    "output_path = \"./results\"         # Directory to save model outputs\n",
    "model_name = \"bert-base-cased\"    # Pre-trained model to use\n",
    "\n",
    "# Training configurations\n",
    "num_epochs = 1\n",
    "learning_rate = 5e-5             # Initial learning rate\n",
    "metric_name = \"f1\"               # Metric to determine the best checkpoint\n",
    "eval_strategy = \"steps\"          # Evaluation frequency based on steps\n",
    "eval_interval = 500              # Number of steps between evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CoNLL 2003 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample from Training Data:\n",
      "Number of sentences in the dataset: 14987\n",
      "Showing the first 5 sentences:\n",
      "\n",
      "Sentence 1:\n",
      "  Token: ['-DOCSTART-', '-X-', '-X-', 'O']\n",
      "\n",
      "\n",
      "Sentence 2:\n",
      "  Token: ['EU', 'NNP', 'B-NP', 'B-ORG']\n",
      "  Token: ['rejects', 'VBZ', 'B-VP', 'O']\n",
      "  Token: ['German', 'JJ', 'B-NP', 'B-MISC']\n",
      "  Token: ['call', 'NN', 'I-NP', 'O']\n",
      "  Token: ['to', 'TO', 'B-VP', 'O']\n",
      "  Token: ['boycott', 'VB', 'I-VP', 'O']\n",
      "  Token: ['British', 'JJ', 'B-NP', 'B-MISC']\n",
      "  Token: ['lamb', 'NN', 'I-NP', 'O']\n",
      "  Token: ['.', '.', 'O', 'O']\n",
      "\n",
      "\n",
      "Sentence 3:\n",
      "  Token: ['Peter', 'NNP', 'B-NP', 'B-PER']\n",
      "  Token: ['Blackburn', 'NNP', 'I-NP', 'I-PER']\n",
      "\n",
      "\n",
      "Sentence 4:\n",
      "  Token: ['BRUSSELS', 'NNP', 'B-NP', 'B-LOC']\n",
      "  Token: ['1996-08-22', 'CD', 'I-NP', 'O']\n",
      "\n",
      "\n",
      "Sentence 5:\n",
      "  Token: ['The', 'DT', 'B-NP', 'O']\n",
      "  Token: ['European', 'NNP', 'I-NP', 'B-ORG']\n",
      "  Token: ['Commission', 'NNP', 'I-NP', 'I-ORG']\n",
      "  Token: ['said', 'VBD', 'B-VP', 'O']\n",
      "  Token: ['on', 'IN', 'B-PP', 'O']\n",
      "  Token: ['Thursday', 'NNP', 'B-NP', 'O']\n",
      "  Token: ['it', 'PRP', 'B-NP', 'O']\n",
      "  Token: ['disagreed', 'VBD', 'B-VP', 'O']\n",
      "  Token: ['with', 'IN', 'B-PP', 'O']\n",
      "  Token: ['German', 'JJ', 'B-NP', 'B-MISC']\n",
      "  Token: ['advice', 'NN', 'I-NP', 'O']\n",
      "  Token: ['to', 'TO', 'B-PP', 'O']\n",
      "  Token: ['consumers', 'NNS', 'B-NP', 'O']\n",
      "  Token: ['to', 'TO', 'B-VP', 'O']\n",
      "  Token: ['shun', 'VB', 'I-VP', 'O']\n",
      "  Token: ['British', 'JJ', 'B-NP', 'B-MISC']\n",
      "  Token: ['lamb', 'NN', 'I-NP', 'O']\n",
      "  Token: ['until', 'IN', 'B-SBAR', 'O']\n",
      "  Token: ['scientists', 'NNS', 'B-NP', 'O']\n",
      "  Token: ['determine', 'VBP', 'B-VP', 'O']\n",
      "  Token: ['whether', 'IN', 'B-SBAR', 'O']\n",
      "  Token: ['mad', 'JJ', 'B-NP', 'O']\n",
      "  Token: ['cow', 'NN', 'I-NP', 'O']\n",
      "  Token: ['disease', 'NN', 'I-NP', 'O']\n",
      "  Token: ['can', 'MD', 'B-VP', 'O']\n",
      "  Token: ['be', 'VB', 'I-VP', 'O']\n",
      "  Token: ['transmitted', 'VBN', 'I-VP', 'O']\n",
      "  Token: ['to', 'TO', 'B-PP', 'O']\n",
      "  Token: ['sheep', 'NN', 'B-NP', 'O']\n",
      "  Token: ['.', '.', 'O', 'O']\n",
      "\n",
      "\n",
      "Sample from Validation Data:\n",
      "Number of sentences in the dataset: 3466\n",
      "Showing the first 5 sentences:\n",
      "\n",
      "Sentence 1:\n",
      "  Token: ['-DOCSTART-', '-X-', '-X-', 'O']\n",
      "\n",
      "\n",
      "Sentence 2:\n",
      "  Token: ['CRICKET', 'NNP', 'B-NP', 'O']\n",
      "  Token: ['-', ':', 'O', 'O']\n",
      "  Token: ['LEICESTERSHIRE', 'NNP', 'B-NP', 'B-ORG']\n",
      "  Token: ['TAKE', 'NNP', 'I-NP', 'O']\n",
      "  Token: ['OVER', 'IN', 'B-PP', 'O']\n",
      "  Token: ['AT', 'NNP', 'B-NP', 'O']\n",
      "  Token: ['TOP', 'NNP', 'I-NP', 'O']\n",
      "  Token: ['AFTER', 'NNP', 'I-NP', 'O']\n",
      "  Token: ['INNINGS', 'NNP', 'I-NP', 'O']\n",
      "  Token: ['VICTORY', 'NN', 'I-NP', 'O']\n",
      "  Token: ['.', '.', 'O', 'O']\n",
      "\n",
      "\n",
      "Sentence 3:\n",
      "  Token: ['LONDON', 'NNP', 'B-NP', 'B-LOC']\n",
      "  Token: ['1996-08-30', 'CD', 'I-NP', 'O']\n",
      "\n",
      "\n",
      "Sentence 4:\n",
      "  Token: ['West', 'NNP', 'B-NP', 'B-MISC']\n",
      "  Token: ['Indian', 'NNP', 'I-NP', 'I-MISC']\n",
      "  Token: ['all-rounder', 'NN', 'I-NP', 'O']\n",
      "  Token: ['Phil', 'NNP', 'I-NP', 'B-PER']\n",
      "  Token: ['Simmons', 'NNP', 'I-NP', 'I-PER']\n",
      "  Token: ['took', 'VBD', 'B-VP', 'O']\n",
      "  Token: ['four', 'CD', 'B-NP', 'O']\n",
      "  Token: ['for', 'IN', 'B-PP', 'O']\n",
      "  Token: ['38', 'CD', 'B-NP', 'O']\n",
      "  Token: ['on', 'IN', 'B-PP', 'O']\n",
      "  Token: ['Friday', 'NNP', 'B-NP', 'O']\n",
      "  Token: ['as', 'IN', 'B-PP', 'O']\n",
      "  Token: ['Leicestershire', 'NNP', 'B-NP', 'B-ORG']\n",
      "  Token: ['beat', 'VBD', 'B-VP', 'O']\n",
      "  Token: ['Somerset', 'NNP', 'B-NP', 'B-ORG']\n",
      "  Token: ['by', 'IN', 'B-PP', 'O']\n",
      "  Token: ['an', 'DT', 'B-NP', 'O']\n",
      "  Token: ['innings', 'NN', 'I-NP', 'O']\n",
      "  Token: ['and', 'CC', 'O', 'O']\n",
      "  Token: ['39', 'CD', 'B-NP', 'O']\n",
      "  Token: ['runs', 'NNS', 'I-NP', 'O']\n",
      "  Token: ['in', 'IN', 'B-PP', 'O']\n",
      "  Token: ['two', 'CD', 'B-NP', 'O']\n",
      "  Token: ['days', 'NNS', 'I-NP', 'O']\n",
      "  Token: ['to', 'TO', 'B-VP', 'O']\n",
      "  Token: ['take', 'VB', 'I-VP', 'O']\n",
      "  Token: ['over', 'IN', 'B-PP', 'O']\n",
      "  Token: ['at', 'IN', 'B-PP', 'O']\n",
      "  Token: ['the', 'DT', 'B-NP', 'O']\n",
      "  Token: ['head', 'NN', 'I-NP', 'O']\n",
      "  Token: ['of', 'IN', 'B-PP', 'O']\n",
      "  Token: ['the', 'DT', 'B-NP', 'O']\n",
      "  Token: ['county', 'NN', 'I-NP', 'O']\n",
      "  Token: ['championship', 'NN', 'I-NP', 'O']\n",
      "  Token: ['.', '.', 'O', 'O']\n",
      "\n",
      "\n",
      "Sentence 5:\n",
      "  Token: ['Their', 'PRP$', 'B-NP', 'O']\n",
      "  Token: ['stay', 'NN', 'I-NP', 'O']\n",
      "  Token: ['on', 'IN', 'B-PP', 'O']\n",
      "  Token: ['top', 'NN', 'B-NP', 'O']\n",
      "  Token: [',', ',', 'O', 'O']\n",
      "  Token: ['though', 'RB', 'B-ADVP', 'O']\n",
      "  Token: [',', ',', 'O', 'O']\n",
      "  Token: ['may', 'MD', 'B-VP', 'O']\n",
      "  Token: ['be', 'VB', 'I-VP', 'O']\n",
      "  Token: ['short-lived', 'JJ', 'B-ADJP', 'O']\n",
      "  Token: ['as', 'IN', 'B-PP', 'O']\n",
      "  Token: ['title', 'NN', 'B-NP', 'O']\n",
      "  Token: ['rivals', 'NNS', 'I-NP', 'O']\n",
      "  Token: ['Essex', 'NNP', 'I-NP', 'B-ORG']\n",
      "  Token: [',', ',', 'O', 'O']\n",
      "  Token: ['Derbyshire', 'NNP', 'B-NP', 'B-ORG']\n",
      "  Token: ['and', 'CC', 'I-NP', 'O']\n",
      "  Token: ['Surrey', 'NNP', 'I-NP', 'B-ORG']\n",
      "  Token: ['all', 'DT', 'O', 'O']\n",
      "  Token: ['closed', 'VBD', 'B-VP', 'O']\n",
      "  Token: ['in', 'RP', 'B-PRT', 'O']\n",
      "  Token: ['on', 'IN', 'B-PP', 'O']\n",
      "  Token: ['victory', 'NN', 'B-NP', 'O']\n",
      "  Token: ['while', 'IN', 'B-SBAR', 'O']\n",
      "  Token: ['Kent', 'NNP', 'B-NP', 'B-ORG']\n",
      "  Token: ['made', 'VBD', 'B-VP', 'O']\n",
      "  Token: ['up', 'RP', 'B-PRT', 'O']\n",
      "  Token: ['for', 'IN', 'B-PP', 'O']\n",
      "  Token: ['lost', 'VBN', 'B-NP', 'O']\n",
      "  Token: ['time', 'NN', 'I-NP', 'O']\n",
      "  Token: ['in', 'IN', 'B-PP', 'O']\n",
      "  Token: ['their', 'PRP$', 'B-NP', 'O']\n",
      "  Token: ['rain-affected', 'JJ', 'I-NP', 'O']\n",
      "  Token: ['match', 'NN', 'I-NP', 'O']\n",
      "  Token: ['against', 'IN', 'B-PP', 'O']\n",
      "  Token: ['Nottinghamshire', 'NNP', 'B-NP', 'B-ORG']\n",
      "  Token: ['.', '.', 'O', 'O']\n",
      "\n",
      "\n",
      "Sample from Test Data:\n",
      "Number of sentences in the dataset: 3684\n",
      "Showing the first 5 sentences:\n",
      "\n",
      "Sentence 1:\n",
      "  Token: ['-DOCSTART-', '-X-', '-X-', 'O']\n",
      "\n",
      "\n",
      "Sentence 2:\n",
      "  Token: ['SOCCER', 'NN', 'B-NP', 'O']\n",
      "  Token: ['-', ':', 'O', 'O']\n",
      "  Token: ['JAPAN', 'NNP', 'B-NP', 'B-LOC']\n",
      "  Token: ['GET', 'VB', 'B-VP', 'O']\n",
      "  Token: ['LUCKY', 'NNP', 'B-NP', 'O']\n",
      "  Token: ['WIN', 'NNP', 'I-NP', 'O']\n",
      "  Token: [',', ',', 'O', 'O']\n",
      "  Token: ['CHINA', 'NNP', 'B-NP', 'B-PER']\n",
      "  Token: ['IN', 'IN', 'B-PP', 'O']\n",
      "  Token: ['SURPRISE', 'DT', 'B-NP', 'O']\n",
      "  Token: ['DEFEAT', 'NN', 'I-NP', 'O']\n",
      "  Token: ['.', '.', 'O', 'O']\n",
      "\n",
      "\n",
      "Sentence 3:\n",
      "  Token: ['Nadim', 'NNP', 'B-NP', 'B-PER']\n",
      "  Token: ['Ladki', 'NNP', 'I-NP', 'I-PER']\n",
      "\n",
      "\n",
      "Sentence 4:\n",
      "  Token: ['AL-AIN', 'NNP', 'B-NP', 'B-LOC']\n",
      "  Token: [',', ',', 'O', 'O']\n",
      "  Token: ['United', 'NNP', 'B-NP', 'B-LOC']\n",
      "  Token: ['Arab', 'NNP', 'I-NP', 'I-LOC']\n",
      "  Token: ['Emirates', 'NNPS', 'I-NP', 'I-LOC']\n",
      "  Token: ['1996-12-06', 'CD', 'I-NP', 'O']\n",
      "\n",
      "\n",
      "Sentence 5:\n",
      "  Token: ['Japan', 'NNP', 'B-NP', 'B-LOC']\n",
      "  Token: ['began', 'VBD', 'B-VP', 'O']\n",
      "  Token: ['the', 'DT', 'B-NP', 'O']\n",
      "  Token: ['defence', 'NN', 'I-NP', 'O']\n",
      "  Token: ['of', 'IN', 'B-PP', 'O']\n",
      "  Token: ['their', 'PRP$', 'B-NP', 'O']\n",
      "  Token: ['Asian', 'JJ', 'I-NP', 'B-MISC']\n",
      "  Token: ['Cup', 'NNP', 'I-NP', 'I-MISC']\n",
      "  Token: ['title', 'NN', 'I-NP', 'O']\n",
      "  Token: ['with', 'IN', 'B-PP', 'O']\n",
      "  Token: ['a', 'DT', 'B-NP', 'O']\n",
      "  Token: ['lucky', 'JJ', 'I-NP', 'O']\n",
      "  Token: ['2-1', 'CD', 'I-NP', 'O']\n",
      "  Token: ['win', 'VBP', 'B-VP', 'O']\n",
      "  Token: ['against', 'IN', 'B-PP', 'O']\n",
      "  Token: ['Syria', 'NNP', 'B-NP', 'B-LOC']\n",
      "  Token: ['in', 'IN', 'B-PP', 'O']\n",
      "  Token: ['a', 'DT', 'B-NP', 'O']\n",
      "  Token: ['Group', 'NNP', 'I-NP', 'O']\n",
      "  Token: ['C', 'NNP', 'I-NP', 'O']\n",
      "  Token: ['championship', 'NN', 'I-NP', 'O']\n",
      "  Token: ['match', 'NN', 'I-NP', 'O']\n",
      "  Token: ['on', 'IN', 'B-PP', 'O']\n",
      "  Token: ['Friday', 'NNP', 'B-NP', 'O']\n",
      "  Token: ['.', '.', 'O', 'O']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load CoNLL 2003 dataset\n",
    "train_data = read_conll_file(os.path.join(data_path, \"eng.train\"))\n",
    "validation_data = read_conll_file(os.path.join(data_path, \"eng.testa\"))\n",
    "test_data = read_conll_file(os.path.join(data_path, \"eng.testb\"))\n",
    "\n",
    "# Display sample data from each dataset\n",
    "print(\"Sample from Training Data:\")\n",
    "print_sample_data(train_data)\n",
    "\n",
    "print(\"Sample from Validation Data:\")\n",
    "print_sample_data(validation_data)\n",
    "\n",
    "print(\"Sample from Test Data:\")\n",
    "print_sample_data(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Label Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: B-LOC\n",
      "1: B-MISC\n",
      "2: B-ORG\n",
      "3: B-PER\n",
      "4: I-LOC\n",
      "5: I-MISC\n",
      "6: I-ORG\n",
      "7: I-PER\n",
      "8: O\n"
     ]
    }
   ],
   "source": [
    "# Extract all labels from the training data\n",
    "labels = [token[3] for sentence in train_data for token in sentence]\n",
    "\n",
    "# Create a sorted list of unique labels and map them to integer indices\n",
    "unique_labels = sorted(set(labels))\n",
    "label_map = {label: i for i, label in enumerate(unique_labels)}\n",
    "\n",
    "# Display the label map\n",
    "for label, index in label_map.items():\n",
    "    print(f\"{index}: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Data to Hugging Face Dataset Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample from Training Data:\n",
      "Dataset contains 14987 samples.\n",
      "\n",
      "Sentence 1:\n",
      "  Tokens: ['-DOCSTART-']\n",
      "  NER Tags: [8]\n",
      "\n",
      "Sentence 2:\n",
      "  Tokens: ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
      "  NER Tags: [2, 8, 1, 8, 8, 8, 1, 8, 8]\n",
      "\n",
      "Sentence 3:\n",
      "  Tokens: ['Peter', 'Blackburn']\n",
      "  NER Tags: [3, 7]\n",
      "\n",
      "Sentence 4:\n",
      "  Tokens: ['BRUSSELS', '1996-08-22']\n",
      "  NER Tags: [0, 8]\n",
      "\n",
      "Sentence 5:\n",
      "  Tokens: ['The', 'European', 'Commission', 'said', 'on', 'Thursday', 'it', 'disagreed', 'with', 'German', 'advice', 'to', 'consumers', 'to', 'shun', 'British', 'lamb', 'until', 'scientists', 'determine', 'whether', 'mad', 'cow', 'disease', 'can', 'be', 'transmitted', 'to', 'sheep', '.']\n",
      "  NER Tags: [8, 2, 6, 8, 8, 8, 8, 8, 8, 1, 8, 8, 8, 8, 8, 1, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "\n",
      "Sample from Validation Data:\n",
      "Dataset contains 3466 samples.\n",
      "\n",
      "Sentence 1:\n",
      "  Tokens: ['-DOCSTART-']\n",
      "  NER Tags: [8]\n",
      "\n",
      "Sentence 2:\n",
      "  Tokens: ['CRICKET', '-', 'LEICESTERSHIRE', 'TAKE', 'OVER', 'AT', 'TOP', 'AFTER', 'INNINGS', 'VICTORY', '.']\n",
      "  NER Tags: [8, 8, 2, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "\n",
      "Sentence 3:\n",
      "  Tokens: ['LONDON', '1996-08-30']\n",
      "  NER Tags: [0, 8]\n",
      "\n",
      "Sentence 4:\n",
      "  Tokens: ['West', 'Indian', 'all-rounder', 'Phil', 'Simmons', 'took', 'four', 'for', '38', 'on', 'Friday', 'as', 'Leicestershire', 'beat', 'Somerset', 'by', 'an', 'innings', 'and', '39', 'runs', 'in', 'two', 'days', 'to', 'take', 'over', 'at', 'the', 'head', 'of', 'the', 'county', 'championship', '.']\n",
      "  NER Tags: [1, 5, 8, 3, 7, 8, 8, 8, 8, 8, 8, 8, 2, 8, 2, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "\n",
      "Sentence 5:\n",
      "  Tokens: ['Their', 'stay', 'on', 'top', ',', 'though', ',', 'may', 'be', 'short-lived', 'as', 'title', 'rivals', 'Essex', ',', 'Derbyshire', 'and', 'Surrey', 'all', 'closed', 'in', 'on', 'victory', 'while', 'Kent', 'made', 'up', 'for', 'lost', 'time', 'in', 'their', 'rain-affected', 'match', 'against', 'Nottinghamshire', '.']\n",
      "  NER Tags: [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 2, 8, 2, 8, 2, 8, 8, 8, 8, 8, 8, 2, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 2, 8]\n",
      "\n",
      "Sample from Test Data:\n",
      "Dataset contains 3684 samples.\n",
      "\n",
      "Sentence 1:\n",
      "  Tokens: ['-DOCSTART-']\n",
      "  NER Tags: [8]\n",
      "\n",
      "Sentence 2:\n",
      "  Tokens: ['SOCCER', '-', 'JAPAN', 'GET', 'LUCKY', 'WIN', ',', 'CHINA', 'IN', 'SURPRISE', 'DEFEAT', '.']\n",
      "  NER Tags: [8, 8, 0, 8, 8, 8, 8, 3, 8, 8, 8, 8]\n",
      "\n",
      "Sentence 3:\n",
      "  Tokens: ['Nadim', 'Ladki']\n",
      "  NER Tags: [3, 7]\n",
      "\n",
      "Sentence 4:\n",
      "  Tokens: ['AL-AIN', ',', 'United', 'Arab', 'Emirates', '1996-12-06']\n",
      "  NER Tags: [0, 8, 0, 4, 4, 8]\n",
      "\n",
      "Sentence 5:\n",
      "  Tokens: ['Japan', 'began', 'the', 'defence', 'of', 'their', 'Asian', 'Cup', 'title', 'with', 'a', 'lucky', '2-1', 'win', 'against', 'Syria', 'in', 'a', 'Group', 'C', 'championship', 'match', 'on', 'Friday', '.']\n",
      "  NER Tags: [0, 8, 8, 8, 8, 8, 1, 5, 8, 8, 8, 8, 8, 8, 8, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert CoNLL data to Hugging Face Dataset format\n",
    "train_dataset = convert_to_hf_dataset(train_data, label_map)\n",
    "validation_dataset = convert_to_hf_dataset(validation_data, label_map)\n",
    "test_dataset = convert_to_hf_dataset(test_data, label_map)\n",
    "\n",
    "# Combine datasets into a DatasetDict\n",
    "datasets = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": validation_dataset,\n",
    "    \"test\": test_dataset,\n",
    "})\n",
    "\n",
    "# Display processed data\n",
    "print(\"Sample from Training Data:\")\n",
    "print_hf_dataset(train_dataset)\n",
    "\n",
    "print(\"Sample from Validation Data:\")\n",
    "print_hf_dataset(validation_dataset)\n",
    "\n",
    "print(\"Sample from Test Data:\")\n",
    "print_hf_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and Align Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eefb698823984b78b08c86e991829c3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14987 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb1b6ff292ce40839b86988b2ea362e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3466 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0258eebe6be4469b61c022b81c1faf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3684 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 14987\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 3466\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 3684\n",
      "    })\n",
      "})\n",
      "\n",
      "\n",
      "Sample from Training Data:\n",
      "Sentence 1:\n",
      "  Original Tokens: ['-DOCSTART-']\n",
      "  Original NER Tags: [8]\n",
      "  Tokenized Input IDs: [101, 118, 141, 9244, 9272, 12426, 1942, 118, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Aligned Labels (numeric): [-100, 8, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "\n",
      "Sentence 2:\n",
      "  Original Tokens: ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
      "  Original NER Tags: [2, 8, 1, 8, 8, 8, 1, 8, 8]\n",
      "  Tokenized Input IDs: [101, 7270, 22961, 1528, 1840, 1106, 21423, 1418, 2495, 12913, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Aligned Labels (numeric): [-100, 2, 8, 1, 8, 8, 8, 1, 8, -100, 8, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "\n",
      "Sentence 3:\n",
      "  Original Tokens: ['Peter', 'Blackburn']\n",
      "  Original NER Tags: [3, 7]\n",
      "  Tokenized Input IDs: [101, 1943, 14428, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Attention Mask: [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Aligned Labels (numeric): [-100, 3, 7, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "\n",
      "Sentence 4:\n",
      "  Original Tokens: ['BRUSSELS', '1996-08-22']\n",
      "  Original NER Tags: [0, 8]\n",
      "  Tokenized Input IDs: [101, 26660, 13329, 12649, 15928, 1820, 118, 4775, 118, 1659, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Aligned Labels (numeric): [-100, 0, -100, -100, -100, 8, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "\n",
      "Sentence 5:\n",
      "  Original Tokens: ['The', 'European', 'Commission', 'said', 'on', 'Thursday', 'it', 'disagreed', 'with', 'German', 'advice', 'to', 'consumers', 'to', 'shun', 'British', 'lamb', 'until', 'scientists', 'determine', 'whether', 'mad', 'cow', 'disease', 'can', 'be', 'transmitted', 'to', 'sheep', '.']\n",
      "  Original NER Tags: [8, 2, 6, 8, 8, 8, 8, 8, 8, 1, 8, 8, 8, 8, 8, 1, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "  Tokenized Input IDs: [101, 1109, 1735, 2827, 1163, 1113, 9170, 1122, 19786, 1114, 1528, 5566, 1106, 11060, 1106, 188, 17315, 1418, 2495, 12913, 1235, 6479, 4959, 2480, 6340, 13991, 3653, 1169, 1129, 12086, 1106, 8892, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Aligned Labels (numeric): [-100, 8, 2, 6, 8, 8, 8, 8, 8, 8, 1, 8, 8, 8, 8, 8, -100, 1, 8, -100, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "\n",
      "Sample from Validation Data:\n",
      "Sentence 1:\n",
      "  Original Tokens: ['-DOCSTART-']\n",
      "  Original NER Tags: [8]\n",
      "  Tokenized Input IDs: [101, 118, 141, 9244, 9272, 12426, 1942, 118, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Aligned Labels (numeric): [-100, 8, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "\n",
      "Sentence 2:\n",
      "  Original Tokens: ['CRICKET', '-', 'LEICESTERSHIRE', 'TAKE', 'OVER', 'AT', 'TOP', 'AFTER', 'INNINGS', 'VICTORY', '.']\n",
      "  Original NER Tags: [8, 8, 2, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "  Tokenized Input IDs: [101, 15531, 9741, 22441, 1942, 118, 149, 27514, 10954, 9272, 9637, 1708, 3048, 18172, 2036, 157, 1592, 22441, 152, 17145, 2069, 13020, 16972, 2101, 138, 26321, 9637, 15969, 27451, 11780, 1708, 7118, 16647, 9565, 3663, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Aligned Labels (numeric): [-100, 8, -100, -100, -100, 8, 2, -100, -100, -100, -100, -100, -100, -100, -100, 8, -100, -100, 8, -100, -100, 8, 8, -100, 8, -100, -100, 8, -100, -100, -100, 8, -100, -100, -100, 8, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "\n",
      "Sentence 3:\n",
      "  Original Tokens: ['LONDON', '1996-08-30']\n",
      "  Original NER Tags: [0, 8]\n",
      "  Tokenized Input IDs: [101, 149, 11414, 2137, 11414, 1820, 118, 4775, 118, 1476, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Aligned Labels (numeric): [-100, 0, -100, -100, -100, 8, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "\n",
      "Sentence 4:\n",
      "  Original Tokens: ['West', 'Indian', 'all-rounder', 'Phil', 'Simmons', 'took', 'four', 'for', '38', 'on', 'Friday', 'as', 'Leicestershire', 'beat', 'Somerset', 'by', 'an', 'innings', 'and', '39', 'runs', 'in', 'two', 'days', 'to', 'take', 'over', 'at', 'the', 'head', 'of', 'the', 'county', 'championship', '.']\n",
      "  Original NER Tags: [1, 5, 8, 3, 7, 8, 8, 8, 8, 8, 8, 8, 2, 8, 2, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "  Tokenized Input IDs: [101, 1537, 1890, 1155, 118, 1668, 1200, 5676, 14068, 1261, 1300, 1111, 3383, 1113, 5286, 1112, 21854, 3222, 8860, 1118, 1126, 6687, 1105, 3614, 2326, 1107, 1160, 1552, 1106, 1321, 1166, 1120, 1103, 1246, 1104, 1103, 2514, 2899, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Aligned Labels (numeric): [-100, 1, 5, 8, -100, -100, -100, 3, 7, 8, 8, 8, 8, 8, 8, 8, 2, 8, 2, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "\n",
      "Sentence 5:\n",
      "  Original Tokens: ['Their', 'stay', 'on', 'top', ',', 'though', ',', 'may', 'be', 'short-lived', 'as', 'title', 'rivals', 'Essex', ',', 'Derbyshire', 'and', 'Surrey', 'all', 'closed', 'in', 'on', 'victory', 'while', 'Kent', 'made', 'up', 'for', 'lost', 'time', 'in', 'their', 'rain-affected', 'match', 'against', 'Nottinghamshire', '.']\n",
      "  Original NER Tags: [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 2, 8, 2, 8, 2, 8, 8, 8, 8, 8, 8, 2, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 2, 8]\n",
      "  Tokenized Input IDs: [101, 2397, 2215, 1113, 1499, 117, 1463, 117, 1336, 1129, 1603, 118, 2077, 1112, 1641, 9521, 8493, 117, 15964, 1105, 9757, 1155, 1804, 1107, 1113, 2681, 1229, 5327, 1189, 1146, 1111, 1575, 1159, 1107, 1147, 4458, 118, 4634, 1801, 1222, 21942, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Aligned Labels (numeric): [-100, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, -100, -100, 8, 8, 8, 2, 8, 2, 8, 2, 8, 8, 8, 8, 8, 8, 2, 8, 8, 8, 8, 8, 8, 8, 8, -100, -100, 8, 8, 2, 8, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "\n",
      "Sample from Test Data:\n",
      "Sentence 1:\n",
      "  Original Tokens: ['-DOCSTART-']\n",
      "  Original NER Tags: [8]\n",
      "  Tokenized Input IDs: [101, 118, 141, 9244, 9272, 12426, 1942, 118, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Aligned Labels (numeric): [-100, 8, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "\n",
      "Sentence 2:\n",
      "  Original Tokens: ['SOCCER', '-', 'JAPAN', 'GET', 'LUCKY', 'WIN', ',', 'CHINA', 'IN', 'SURPRISE', 'DEFEAT', '.']\n",
      "  Original NER Tags: [8, 8, 0, 8, 8, 8, 8, 3, 8, 8, 8, 8]\n",
      "  Tokenized Input IDs: [101, 156, 9244, 10954, 2069, 118, 147, 12240, 14962, 25075, 1942, 149, 21986, 2428, 3663, 160, 11607, 117, 24890, 11607, 1592, 15969, 156, 19556, 22861, 6258, 2036, 18581, 2271, 12420, 1942, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Aligned Labels (numeric): [-100, 8, -100, -100, -100, 8, 0, -100, -100, 8, -100, 8, -100, -100, -100, 8, -100, 8, 3, -100, -100, 8, 8, -100, -100, -100, -100, 8, -100, -100, -100, 8, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "\n",
      "Sentence 3:\n",
      "  Original Tokens: ['Nadim', 'Ladki']\n",
      "  Original NER Tags: [3, 7]\n",
      "  Tokenized Input IDs: [101, 11896, 3309, 1306, 2001, 1181, 2293, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Aligned Labels (numeric): [-100, 3, -100, -100, 7, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "\n",
      "Sentence 4:\n",
      "  Original Tokens: ['AL-AIN', ',', 'United', 'Arab', 'Emirates', '1996-12-06']\n",
      "  Original NER Tags: [0, 8, 0, 4, 4, 8]\n",
      "  Tokenized Input IDs: [101, 18589, 118, 19016, 2249, 117, 1244, 4699, 14832, 1820, 118, 1367, 118, 5037, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Aligned Labels (numeric): [-100, 0, -100, -100, -100, 8, 0, 4, 4, 8, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "\n",
      "Sentence 5:\n",
      "  Original Tokens: ['Japan', 'began', 'the', 'defence', 'of', 'their', 'Asian', 'Cup', 'title', 'with', 'a', 'lucky', '2-1', 'win', 'against', 'Syria', 'in', 'a', 'Group', 'C', 'championship', 'match', 'on', 'Friday', '.']\n",
      "  Original NER Tags: [0, 8, 8, 8, 8, 8, 1, 5, 8, 8, 8, 8, 8, 8, 8, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "  Tokenized Input IDs: [101, 1999, 1310, 1103, 6465, 1104, 1147, 3141, 1635, 1641, 1114, 170, 6918, 123, 118, 122, 1782, 1222, 7303, 1107, 170, 1990, 140, 2899, 1801, 1113, 5286, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Aligned Labels (numeric): [-100, 0, 8, 8, 8, 8, 8, 1, 5, 8, 8, 8, 8, 8, -100, -100, 8, 8, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(label_map))\n",
    "\n",
    "# Tokenize and align labels using partial for convenience\n",
    "tokenize_and_align_labels = partial(tokenize_and_align_labels, tokenizer=tokenizer)\n",
    "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# Display tokenized datasets\n",
    "print(\"Tokenized Datasets:\\n\", tokenized_datasets, end='\\n\\n')\n",
    "\n",
    "print(\"Sample from Training Data:\")\n",
    "print_tokenized_dataset(tokenized_datasets['train'])\n",
    "\n",
    "print(\"Sample from Validation Data:\")\n",
    "print_tokenized_dataset(tokenized_datasets['validation'])\n",
    "\n",
    "print(\"Sample from Test Data:\")\n",
    "print_tokenized_dataset(tokenized_datasets['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up TrainingArguments and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71767a9fa65f4b7e91da4059136d0fc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3914, 'grad_norm': 2.698897123336792, 'learning_rate': 4.7331910352187837e-05, 'epoch': 0.05}\n",
      "{'loss': 0.1227, 'grad_norm': 7.496398448944092, 'learning_rate': 4.466382070437567e-05, 'epoch': 0.11}\n",
      "{'loss': 0.0919, 'grad_norm': 0.6311567425727844, 'learning_rate': 4.1995731056563505e-05, 'epoch': 0.16}\n",
      "{'loss': 0.0784, 'grad_norm': 4.44400691986084, 'learning_rate': 3.932764140875134e-05, 'epoch': 0.21}\n",
      "{'loss': 0.0832, 'grad_norm': 2.786243438720703, 'learning_rate': 3.665955176093917e-05, 'epoch': 0.27}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5774ded47dda4c0499360fcdfb490504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/434 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.94      0.95      0.94      1837\n",
      "        MISC       0.76      0.84      0.80       922\n",
      "         ORG       0.90      0.85      0.87      1341\n",
      "         PER       0.97      0.96      0.96      1842\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      5942\n",
      "   macro avg       0.89      0.90      0.89      5942\n",
      "weighted avg       0.91      0.91      0.91      5942\n",
      "\" of type <class 'str'> for key \"eval/classification_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0639970526099205, 'eval_precision': 0.9094664821643778, 'eval_recall': 0.9131605520026927, 'eval_f1': 0.9108744127047756, 'eval_classification_report': '              precision    recall  f1-score   support\\n\\n         LOC       0.94      0.95      0.94      1837\\n        MISC       0.76      0.84      0.80       922\\n         ORG       0.90      0.85      0.87      1341\\n         PER       0.97      0.96      0.96      1842\\n\\n   micro avg       0.91      0.91      0.91      5942\\n   macro avg       0.89      0.90      0.89      5942\\nweighted avg       0.91      0.91      0.91      5942\\n', 'eval_runtime': 16.8115, 'eval_samples_per_second': 206.169, 'eval_steps_per_second': 25.816, 'epoch': 0.27}\n",
      "{'loss': 0.0695, 'grad_norm': 4.252267360687256, 'learning_rate': 3.3991462113127e-05, 'epoch': 0.32}\n",
      "{'loss': 0.0537, 'grad_norm': 4.346560001373291, 'learning_rate': 3.1323372465314835e-05, 'epoch': 0.37}\n",
      "{'loss': 0.0531, 'grad_norm': 0.05698142573237419, 'learning_rate': 2.8655282817502672e-05, 'epoch': 0.43}\n",
      "{'loss': 0.051, 'grad_norm': 5.400124549865723, 'learning_rate': 2.5987193169690503e-05, 'epoch': 0.48}\n",
      "{'loss': 0.0562, 'grad_norm': 2.767338275909424, 'learning_rate': 2.3319103521878334e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e885685ffe4a579a58c96d99039627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/434 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.93      0.97      0.95      1837\n",
      "        MISC       0.89      0.81      0.85       922\n",
      "         ORG       0.87      0.92      0.90      1341\n",
      "         PER       0.96      0.98      0.97      1842\n",
      "\n",
      "   micro avg       0.92      0.94      0.93      5942\n",
      "   macro avg       0.91      0.92      0.92      5942\n",
      "weighted avg       0.92      0.94      0.93      5942\n",
      "\" of type <class 'str'> for key \"eval/classification_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.05810042470693588, 'eval_precision': 0.9202359003595662, 'eval_recall': 0.9362167620329855, 'eval_f1': 0.9276636840418444, 'eval_classification_report': '              precision    recall  f1-score   support\\n\\n         LOC       0.93      0.97      0.95      1837\\n        MISC       0.89      0.81      0.85       922\\n         ORG       0.87      0.92      0.90      1341\\n         PER       0.96      0.98      0.97      1842\\n\\n   micro avg       0.92      0.94      0.93      5942\\n   macro avg       0.91      0.92      0.92      5942\\nweighted avg       0.92      0.94      0.93      5942\\n', 'eval_runtime': 17.7791, 'eval_samples_per_second': 194.948, 'eval_steps_per_second': 24.411, 'epoch': 0.53}\n",
      "{'loss': 0.062, 'grad_norm': 3.221877336502075, 'learning_rate': 2.0651013874066168e-05, 'epoch': 0.59}\n",
      "{'loss': 0.0542, 'grad_norm': 2.4324893951416016, 'learning_rate': 1.7982924226254002e-05, 'epoch': 0.64}\n",
      "{'loss': 0.0452, 'grad_norm': 2.3677196502685547, 'learning_rate': 1.5314834578441836e-05, 'epoch': 0.69}\n",
      "{'loss': 0.0494, 'grad_norm': 2.9166312217712402, 'learning_rate': 1.264674493062967e-05, 'epoch': 0.75}\n",
      "{'loss': 0.0419, 'grad_norm': 0.08017772436141968, 'learning_rate': 9.978655282817503e-06, 'epoch': 0.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c7c0d84ad3442aea322c1d5c481bdb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/434 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.97      0.96      0.96      1837\n",
      "        MISC       0.87      0.90      0.88       922\n",
      "         ORG       0.91      0.92      0.91      1341\n",
      "         PER       0.97      0.97      0.97      1842\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      5942\n",
      "   macro avg       0.93      0.94      0.93      5942\n",
      "weighted avg       0.94      0.94      0.94      5942\n",
      "\" of type <class 'str'> for key \"eval/classification_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.04237821698188782, 'eval_precision': 0.9397867130835945, 'eval_recall': 0.9436216762032985, 'eval_f1': 0.9416518252622397, 'eval_classification_report': '              precision    recall  f1-score   support\\n\\n         LOC       0.97      0.96      0.96      1837\\n        MISC       0.87      0.90      0.88       922\\n         ORG       0.91      0.92      0.91      1341\\n         PER       0.97      0.97      0.97      1842\\n\\n   micro avg       0.94      0.94      0.94      5942\\n   macro avg       0.93      0.94      0.93      5942\\nweighted avg       0.94      0.94      0.94      5942\\n', 'eval_runtime': 18.8097, 'eval_samples_per_second': 184.267, 'eval_steps_per_second': 23.073, 'epoch': 0.8}\n",
      "{'loss': 0.0445, 'grad_norm': 1.8011629581451416, 'learning_rate': 7.310565635005337e-06, 'epoch': 0.85}\n",
      "{'loss': 0.0485, 'grad_norm': 1.1676015853881836, 'learning_rate': 4.6424759871931695e-06, 'epoch': 0.91}\n",
      "{'loss': 0.0496, 'grad_norm': 3.238689422607422, 'learning_rate': 1.9743863393810032e-06, 'epoch': 0.96}\n",
      "{'train_runtime': 359.6904, 'train_samples_per_second': 41.666, 'train_steps_per_second': 5.21, 'train_loss': 0.07855289532000889, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1874, training_loss=0.07855289532000889, metrics={'train_runtime': 359.6904, 'train_samples_per_second': 41.666, 'train_steps_per_second': 5.21, 'total_flos': 960565719981294.0, 'train_loss': 0.07855289532000889, 'epoch': 1.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_path,\n",
    "    eval_strategy=eval_strategy,\n",
    "    eval_steps=eval_interval,\n",
    "    save_steps=eval_interval,\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_steps=100,\n",
    "    learning_rate=learning_rate,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    ")\n",
    "\n",
    "# Define data collator and metrics\n",
    "data_collator = partial(data_collator, tokenizer=tokenizer)\n",
    "compute_metrics = partial(compute_classification_metrics, label_list=unique_labels)\n",
    "\n",
    "# Initialize and train the model using Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Named Entities from Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: John Smith is a software engineer who works at Google.\n",
      "Named Entities: ['John', 'Smith', 'Google'] \n",
      "\n",
      "Example 2: The company Apple Inc. announced its new product, the iPhone 12, at a press conference held in San Francisco.\n",
      "Named Entities: ['Apple', 'Inc', 'iPhone', '12', 'Francisco'] \n",
      "\n",
      "\n",
      "Example 3: The actor Tom Hanks starred in the movie Forrest Gump.\n",
      "Named Entities: ['Tom', 'Hank', '##s', 'Forrest', 'G', '##ump'] \n",
      "\n",
      "\n",
      "Example 4: Paris is the capital city of France.\n",
      "Named Entities: [] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example sentences for NER extraction\n",
    "sentence1 = \"John Smith is a software engineer who works at Google.\"\n",
    "named_entities1 = extract_named_entities(sentence1, tokenizer, model, label_map)\n",
    "print(f\"Example 1: {sentence1}\")\n",
    "print(f\"Named Entities: {named_entities1}\\n\")\n",
    "\n",
    "sentence2 = \"The company Apple Inc. announced its new product, the iPhone 12, at a press conference held in San Francisco.\"\n",
    "named_entities2 = extract_named_entities(sentence2, tokenizer, model, label_map)\n",
    "print(f\"Example 2: {sentence2}\")\n",
    "print(f\"Named Entities: {named_entities2}\\n\")\n",
    "\n",
    "sentence3 = \"The actor Tom Hanks starred in the movie Forrest Gump.\"\n",
    "named_entities3 = extract_named_entities(sentence3, tokenizer, model, label_map)\n",
    "print(f\"Example 3: {sentence3}\")\n",
    "print(f\"Named Entities: {named_entities3}\\n\")\n",
    "\n",
    "sentence4 = \"Paris is the capital city of France.\"\n",
    "named_entities4 = extract_named_entities(sentence4, tokenizer, model, label_map)\n",
    "print(f\"Example 4: {sentence4}\")\n",
    "print(f\"Named Entities: {named_entities4}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
